{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "\n",
    "SRC_PATH = expanduser(\"~\") + '/SageMaker/mastering-ml-on-aws/chapter3/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.context import SparkContext\n",
    "\n",
    "sc = SparkContext('local', 'test')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = sql.read.csv(SRC_PATH + 'train.csv', header=True, inferSchema=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio| black|lstat|medv|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "|  1|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|\n",
      "|  2|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|\n",
      "|  4|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|\n",
      "|  5|0.06905| 0.0| 2.18|   0|0.458|7.147|54.2|6.0622|  3|222|   18.7| 396.9| 5.33|36.2|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "housing_df.show(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "training_features = ['crim', 'zn', 'indus', 'chas', 'nox', 'rm', 'age', 'dis', 'tax', 'ptratio', 'lstat']\n",
    "vector_assembler = VectorAssembler(inputCols=training_features, outputCol=\"features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm| age|   dis|rad|tax|ptratio| black|lstat|medv|            features|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "|  1|0.00632|18.0| 2.31|   0|0.538|6.575|65.2|  4.09|  1|296|   15.3| 396.9| 4.98|24.0|[0.00632,18.0,2.3...|\n",
      "|  2|0.02731| 0.0| 7.07|   0|0.469|6.421|78.9|4.9671|  2|242|   17.8| 396.9| 9.14|21.6|[0.02731,0.0,7.07...|\n",
      "|  4|0.03237| 0.0| 2.18|   0|0.458|6.998|45.8|6.0622|  3|222|   18.7|394.63| 2.94|33.4|[0.03237,0.0,2.18...|\n",
      "+---+-------+----+-----+----+-----+-----+----+------+---+---+-------+------+-----+----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_with_features_vector = vector_assembler.transform(housing_df)\n",
    "df_with_features_vector.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df_with_features_vector.randomSplit([0.8, 0.2], seed=17)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "linear = LinearRegression(featuresCol=\"features\", labelCol=\"medv\")\n",
    "linear_model = linear.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+--------------------+------------------+\n",
      "| ID|   crim|  zn|indus|chas|  nox|   rm|  age|   dis|rad|tax|ptratio| black|lstat|medv|            features|        prediction|\n",
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+--------------------+------------------+\n",
      "|  7|0.08829|12.5| 7.87|   0|0.524|6.012| 66.6|5.5605|  5|311|   15.2| 395.6|12.43|22.9|[0.08829,12.5,7.8...|21.273530243958177|\n",
      "| 24|0.98843| 0.0| 8.14|   0|0.538|5.813|100.0|4.0952|  4|307|   21.0|394.54|19.88|14.5|[0.98843,0.0,8.14...|13.894245541490553|\n",
      "| 44|0.15936| 0.0| 6.91|   0|0.448|6.211|  6.5|5.7209|  3|233|   17.9|394.46| 7.44|24.7|[0.15936,0.0,6.91...|25.209683694484067|\n",
      "+---+-------+----+-----+----+-----+-----+-----+------+---+---+-------+------+-----+----+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions_df = linear_model.transform(test_df)\n",
    "predictions_df.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7086992014223543"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_model.summary.r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6880637937295275"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(labelCol=\"medv\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "evaluator.evaluate(predictions_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "linear = LinearRegression(featuresCol=\"features\", labelCol=\"medv\")\n",
    "pipeline = Pipeline(stages=[vector_assembler, linear])\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(linear.elasticNetParam, [0.01, 0.02, 0.05]) \\\n",
    "    .addGrid(linear.solver, ['normal', 'l-bfgs']) \\\n",
    "    .addGrid(linear.regParam, [0.4, 0.5, 0.6]).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=param_grid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=10)\n",
    "\n",
    "optimized_model = crossval.fit(housing_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('epsilon', 1.35),\n",
       " ('featuresCol', 'features'),\n",
       " ('predictionCol', 'prediction'),\n",
       " ('loss', 'squaredError'),\n",
       " ('elasticNetParam', 0.02),\n",
       " ('regParam', 0.6),\n",
       " ('maxIter', 100),\n",
       " ('labelCol', 'medv'),\n",
       " ('tol', 1e-06),\n",
       " ('standardization', True),\n",
       " ('aggregationDepth', 2),\n",
       " ('fitIntercept', True),\n",
       " ('solver', 'l-bfgs')]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k.name, v) for (k, v) in optimized_model.bestModel.stages[1].extractParamMap().items()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.60228046689935,\n",
       " 0.6022857524897973,\n",
       " 0.6023834177393386,\n",
       " 0.6023887410814059,\n",
       " 0.6026432049338516,\n",
       " 0.6026481973755449,\n",
       " 0.6030001230672197,\n",
       " 0.6030052621812348,\n",
       " 0.6031040135770328,\n",
       " 0.6031091439263058,\n",
       " 0.6032064249778748,\n",
       " 0.6032086198971643,\n",
       " 0.60355796933488,\n",
       " 0.6035626800336215,\n",
       " 0.6036587578463659,\n",
       " 0.603663431370495,\n",
       " 0.6034106428627964,\n",
       " 0.6034118340373834]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_model.avgMetrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7202817971205354"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, evaluation_df = housing_df.randomSplit([0.8, 0.2], seed=17)\n",
    "evaluator.evaluate(optimized_model.transform(evaluation_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
